{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and NetCDF climate data\n",
    "Unlike MatLab, Python does not have native support for multidimensional NetCDF datasets. Instead, we have to import a few specific packages into our script and do a few extra steps to manipulate the data. The notebook below demonstrates the steps needed to import, link variables representing the different dimensions, and run simple analyses on netCDF data.  \n",
    "\n",
    "In the example here, we'll use daily precipitation predictions downscaled to a 1/16-degree resolution spanning the years 1990 to 2005. Thus, our data has 4 dimensions: `latitude`, `longitude`, `time`, and `precipitation`. *(Actually, the dataset has a 5th \"dimension\" as well, but this only includes data on the geographic coordinate reference system and has only one value, so we'll ignore it)*. \n",
    "\n",
    "Resources: \n",
    "* http://www.ceda.ac.uk/static/media/uploads/ncas-reading-2015/10_read_netcdf_python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the `.nc` data file into our Python script as netCDF4 `dataset` object\n",
    "The Python *NetCDF4* package allows us to read in our `.nc` file into a NetCDF4 `dataset` object that we can manipulate programmatically. Documentation on the NetCDF4 package is here: http://unidata.github.io/netcdf4-python/, and it displays the various *properties* and *methods* of the `dataset` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package to read netCDF file\n",
    "import netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file into a netCDF dataset object\n",
    "fileName = 'macav2livneh_pr_bcc-csm1-1_r1i1p1_historical_1990_2005_CONUS_monthly.nc'\n",
    "dataset = netCDF4.Dataset(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that the `dataset` variable points to a netCDF4 dataset object\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show some documentation on the `dataset` object\n",
    "?dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore out netCDF dataset\n",
    "With our dataset object created, we can explore a bit about the data within it and its structure. Specifically, we'll examine what <u>dimensions</u>, <u>variables</u>, and <u>attributes</u> are contained within the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dimensions\n",
    "The dimensions in our dataset are accessed as a Python **dictionary**, which is a collection of values referenced by specific keys. Here, each dimension is listed as a *key*, and to get information about that dimension, we \"look up\" its definition in the dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the full dimensions dictionary\n",
    "dataset.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show just the keys, or dimensions, in the dataset\n",
    "dataset.dimensions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the values associated with the `time` dimension\n",
    "dataset.dimensions['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "► *<u>Now you try it</u>: Show the values associated with the \"lat\" dimension. How many items are in this dimension? How many in the 'lon' and 'crs' dimensions?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dimensions[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing up, we see our dataset has 4 dimensions: \n",
    "* `lat` with 444 values\n",
    "* `lon` with 922 values\n",
    "* `time` with 192 values, and \n",
    "* a coordinate reference system (`crs`) with one value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Variables\n",
    "Like dimensions, the dataset's variables are accessed as dictionary objects. First let's list all the variables contained in our datset by exposing the \"keys\" included in the dictionary. \n",
    "Our dataset has 5 variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the 'key' or names of the variables\n",
    "dataset.variables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see many of the same names as the dimensions, but also a *precipitation* variable. \n",
    "* `lat`\n",
    "* `lon`\n",
    "* `time`\n",
    "* `precipitation`\n",
    "* `crs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show attributes of the precipitation variable\n",
    "dataset.variables['precipitation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Attributes\n",
    "NetCDF datasets have both global attributes and attributes associated with each variable. Here's how we explore what attributes are included in each and how to access information on each attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Global attributes\n",
    "Listing the datset's **global** attributes is done bit differently than the process for dimensions and variables. Here, the `ncattrs()` function returns a list of the dataset's global attributes. Then we can display more information on any of these attributes using the `getattr()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the dataset's attributes\n",
    "dataset.ncattrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display information stored in the `summary` attribute\n",
    "dataset.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Variable attributes\n",
    "Now we'll focus on the attributes of a single variable in our datatset. We'll choose the `precipitation` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the precipitation variable's attributes\n",
    "dataset.variables['precipitation'].ncattrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reveal the information associated with the \"cell_methods\" attribute\n",
    "dataset.variables['precipitation'].cell_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There's a lot going on in the above statements. It works, but it can be hard to read for the newbie. One of the advantanges of Python code is its readability, but this only works of coders write the code to be readable. So, let's rewrite the above statement so that it's more readable. It appears less \"efficient\", but sometimes readability is better than terseness.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the precipitation variable into a Python variable called \"precip\"\n",
    "precip = dataset.variables['precipitation']\n",
    "#Now list its attributes\n",
    "precip.ncattrs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `precip` Python variable established, we can use it to display attribute contents directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip._FillValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Working with the data\n",
    "With some familiarity of what's in the dataset, we can now start manipulating and visualizing the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import the variables into netCDF4 `variable` objects\n",
    "Let's now break our dataset into its component variables so that we can work with each more easily. Here, we assign Python variables to the four dataset variables: `time`, `latitute`, `longitude`, and `precipitation`. (We'll ignore the `crs` variable as that contains only one value, a metadata value listing the coordinate system used.) Then, we'll quickly examine some properties of these variable objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the variables in NETCDF file\n",
    "ncTime = dataset.variables['time']\n",
    "ncLon = dataset.variables['lon']\n",
    "ncLat = dataset.variables['lat']\n",
    "ncPrecip = dataset.variables['precipitation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that these objects are netCDF4 variables\n",
    "type(ncPrecip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display what we can do with a variable object\n",
    "?ncPrecip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if we just display everything about the variable?\n",
    "ncPrecip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "► *<u>Now you try it</u>: Recalling how we revealed the attributes of the `precip` variable above, what are the units of the `time` variable? The `lat` and `lon` variables?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the units of the ncTime variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable shape**: These four variables are all <u>arrays</u>, i.e. a series of values set across one or or more dimensions. We can examine the size and dimensions of each variable via its `shape` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncTime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncLat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncLon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncPrecip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the size of the three dimensions of the `precip` variable corresponds to the size of the `time`, `lat`, and `lon` variables, respectively. <u>*This gives a more tangible sense of how these data are structured and how we can manipulate our data.*</u>\n",
    "\n",
    "More specifically, we see that the data in the `precip` variable is 3 dimensional. The first dimension is time, and the other two dimensions are x-y coordinates in space. So we can <u>envision our data as a stack of precipitation maps, with each layer in the stack as precipitation values for a single time snapshot.</u> This helps us in subsetting our data...\n",
    "\n",
    "What may not be clear here is: *what then are the `time`, `lat`, and `lon` variables, if everything is held in the one `precip` variable?* This has everything to do with the fact that the actual data (i.e., the amount of precipitation) are referenced by their *index position* in the array, as we'll see in a moment. It'd be confusing to explain this in more detail at the moment. Instead, let's move a bit further with the data and then we'll come back to this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Extracting data from our variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Extract a single value (i.e. single location-time value)\n",
    "The `precip` variable has 3 axes: `time`,`lat`, and `lon`, with sizes of 192, 444, and 922, respectively. And the value at a given time/geographic coordinate is the predicted rainfall at that specific time/location. We can extract a specific precipitation value by specifying a `time`, `lat`, and `lon` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncPrecip[0,250,300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpolated precipitation at that time/location is `37 mm`. \n",
    "\n",
    "<u>But what time and location did we actually specify??</u> *What is time=0?? Likewise, a latitude of \"250\" and a longitude of \"300\" are not really geographic coordinates.* \n",
    "\n",
    "These values are actually pointers to the **positions** in the precipitation array, that is, the 1st time slice, the 251st \"latitude\" column, and the 301st \"longitude\" column - if you think of our precipitation array as a stack of lat/long tables, with a layer for each time. (Also note that Python indices are zero-based, meaning the values start at zero, not 1.)\n",
    "\n",
    "**So how, then, do we extract data for a known time and/or location?** Well, those data are contained in the other arrays, with positions corresponding to the axes in the precipitation array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ncTime[0])\n",
    "print(ncLat[250])\n",
    "print(ncLon[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that the precipitation value extracted above is associated with the location (`40.71824°N`,`254.15625°W`) and the time \"`32864`\"??!?\n",
    "\n",
    "That time value is actually days since 01-01-1990 (found in the datasets metadata):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncTime.units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have tools to conver this into a more readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new array, converting days since 1900 to a date time value\n",
    "ncTime2 = netCDF4.num2date(ncTime[:],ncTime.units)\n",
    "ncTime2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see the first time slice is Dec 12, 1989. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now what we have to do is somehow cross-reference the *values* in the `time2`, `lat`, and `lon` arrays with the *axes* in the precipitation array. In doing so, we can more easily extract values we want and run various analyses on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Python's NumPy (numeric Python) package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Converting the netCDF4 variable into NumPy arrays\n",
    "The process to do this involves converting each of our NetCDF variabls into numeric arrays which then allows us to use Python's **NumPy** package to do exactly what we need to do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the numpy package, calling it \"np\" to save typing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the NetCDF variables to numpy arrays \n",
    "arrTime = ncTime2[:].astype(np.datetime64)\n",
    "arrLat = ncLat[:]\n",
    "arrLon = ncLon[:]\n",
    "arrPrecip = ncPrecip[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the type of object created\n",
    "type(arrPrecip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Working with NumPy arrays\n",
    "The netCDF variables were converted to Numpy *masked arrays*. This means that, in addition to the N-dimensional numeric array, we have an associated n-dimensional array, but one with boolean values that indicate whether the data should be used in calculations or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the shape of the precip array\n",
    "arrPrecip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays work much like the netCDF variables in terms of querying values using their position..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrPrecip[0,250,300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrTime[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrLat[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrLon[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Subsetting our data\n",
    "With our variable stored as NumPy arrays, we can use NumPy methods to slice our data across time and/or space. We'll begin simply by just [blindly] using raw position values (i.e. not actual times or lat/long coordinates) to do first a time series plot and then a simple map for a single time slice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Time series for one location\n",
    "To extract precipitation across all time for one location (here we'll choose the `lat` at position `250` and the lon at postition `300`), we use a colon to say \"grab all values\" at the first axis (`time`), then then specific values for the `lat` and `lon` axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab all the precip data at location lat=200 and lon=300\n",
    "arrPrecipLocX = arrPrecip[:,250,300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the shape of the result: should be 192 \n",
    "arrPrecipLocX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the first 10 values in the resulting array\n",
    "arrPrecipLocX[0:10].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Plotting - with NumPy & <u>MatPlotLib</u>\n",
    "We use another library for plotting. Python has a few, but as you are likely familiar with Matlab, we'll use `matplotlib` which was desinged after Matlab's plotting procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the pyplot subpackage and enable inline plotting\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot as a time series\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "plt.plot(arrPrecipLocX);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot time series for three points long a longitudinal transect\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "plt.plot(arrPrecip[:,250,300])\n",
    "plt.plot(arrPrecip[:,300,300])\n",
    "plt.plot(arrPrecip[:,350,300])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 Creating a location matrix for a single point in time, or average across all times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull precip data for all lat/lon dimensions for the most recent record\n",
    "arrPrecipRecent = arrPrecip[-1,:,:]\n",
    "arrPrecipRecent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or, compute the mean across all time values (time = axis zero)\n",
    "arrPrecipMean = arrPrecip.mean(axis=0)\n",
    "arrPrecipMean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or, compute the mean across all time values (time = axis zero) for a spatial subset\n",
    "arrPrecipMeanSubset = arrPrecip[:,250:300,300:400].mean(axis=0)\n",
    "arrPrecipMeanSubset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.4 Plotting in two dimensions\n",
    "Matplotlib's `imshow` function allows us to plot images, i.e., data stored with X/Y coordinates, as we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(arrPrecipMean,origin=(0,0),cmap=\"YlGnBu\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(arrPrecipMeanSubset,origin=(0,0),cmap=\"YlGnBu\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Pandas\n",
    "**Numpy** works great for computations on n-dimensional arrays, but the axes are still a bit befuddling as they can only be accessed via position, not actual time stamps or actual lat-long coordinates. You can see that easily in the map we just created: the coordinates are *image* coordinates, not geographic ones (e.g. degrees north and degrees east). \n",
    "\n",
    "Numpy has a companion Python package called **Pandas** (a contraction of *Pan*el *Da*ta) that overcomes this issue by allowing us to store and manipulate our data in **data frames**. The key difference [for us] between *Numpy's ndarrays* and *Pandas' dataframes* is the ability to name and label columns and rows so we can move beyond just integer indices. \n",
    "\n",
    "The drawback, however, is that dataframes require data to be stored in two dimensions, i.e., in rows and columns. (There are ways around that, using something called a hierarchical multiindex, but more on that later...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Reducing and converting our NumPy arrays to Pandas dataframes\n",
    "Anyway, let's convert our Numpy arrays to Pandas dataframes. Of course, before we can do that, we need to ***reduce*** our data to two or fewer dimensions. We've done that above, creating the time series dataset (by reducing the lat and lon dimensions to a single location), and by creating the mapped dataset (by reducing the time dimension to a single point in time or by averaging all time values into one value.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Full process from netCDF4 variable to 2d Pandas dataframes\n",
    "Let's review the whole process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the precip variable from the netCDF dataset\n",
    "precip = dataset.variables['precipitation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the netCDF variable to a 3-d numpy array\n",
    "arrPrecip = precip[:]\n",
    "arrPrecip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the 3d array to a 1d array\n",
    "arrTimeSeries = arrPrecip[:,250,300]\n",
    "arrTimeSeries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the 3d array to a 2d array, collapsing the time dimension into average values\n",
    "arrMeanPrecip = arrPrecip.mean(axis=0)\n",
    "arrMeanPrecip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the 1d time series array to a dataframe\n",
    "dfTimeSeries=pd.DataFrame(arrTimeSeries)\n",
    "dfTimeSeries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first 5 records\n",
    "dfTimeSeries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the 2d mean values to a dataframe\n",
    "dfMeanPrecip=pd.DataFrame(arrMeanPrecip)\n",
    "dfMeanPrecip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first 5 records of the \n",
    "dfMeanPrecip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Adding column names and indices to our dataframes\n",
    "***♦ Note that the rows and columns remain as simple integers, not actual times or lat/long coordinates***.\n",
    "\n",
    "By adding row and column labels to our dataset - using the `time`, `lat`, and `lon` variable values to assign those row & column names, we can then manipulate our data using those values (as compared to just integers in numpy arrays). \n",
    "\n",
    "**Note**: row labels are called indices in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we import the variable values in to Pandas *series* objects. Series are just 1d arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the time, lat, and lon variables to Pandas series objects (series b/c 1 dimension)\n",
    "ncTime = dataset.variables['time']\n",
    "seriesTime = pd.Series(netCDF4.num2date(ncTime[:],ncTime.units))\n",
    "seriesLat = pd.Series(dataset.variables['lat'][:])\n",
    "seriesLon = pd.Series(dataset.variables['lon'][:])\n",
    "print(seriesTime.shape,seriesLat.shape,seriesLon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign those values to the dataFrame column name and row indices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the index of the time series dataframe to the actual times\n",
    "dfTimeSeries.index = seriesTime\n",
    "dfTimeSeries.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column names and an index to the dfMeanPrecip dataframe\n",
    "dfMeanPrecip.columns = seriesLon\n",
    "dfMeanPrecip.index = seriesLat\n",
    "dfMeanPrecip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Subsetting data in Pandas dataframes\n",
    "### 5.2.1 Selecting a *slice* of time\n",
    "With our data in Pandas dataframe and the actual times set as the index, we can select, subset, or **slice** our data on *actual times*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the data for the years 2000 to 2004\n",
    "df_subset = dfTimeSeries[(dfTimeSeries.index >= '2000') & \n",
    "                         (dfTimeSeries.index < '2004')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the subset\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "plt.plot(df_subset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "♦ **AND** we get the actual dates on the axis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using `xarray`\n",
    "Ok. We are starting to get a handle on the basics of wrangling climate data with NetCDF4/NumPy/Pandas. However, before going any deeper into this rabbit hole, it's time to move onto something that makes this all MUCH easier. Turns out, someone coded a new Python package called **`xarray`** that includes a number of functions and methods tailored exactly to our needs. \n",
    "\n",
    "So, let's move to the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
